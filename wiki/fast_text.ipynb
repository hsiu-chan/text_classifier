{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from eval.evaluate import accuracy\n",
    "from tensorflow.contrib import slim\n",
    "from loss.loss import cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(utils.Sequence):\n",
    "    def __init__(self, img_paths, batch_size, img_size, shuffle=True, mode='train', aug=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.img_size = img_size\n",
    "        self.mode = mode\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = [p.replace('_training.tif', '_manual1.gif') for p in self.img_paths]\n",
    "        self.aug = aug\n",
    "        self.indexes = np.arange(len(self.mask_paths))\n",
    "        # data augmentation\n",
    "        self.seq = iaa.Sequential([\n",
    "            iaa.Fliplr(0.5), # 50% horizontal flip\n",
    "            iaa.Affine(\n",
    "                rotate=(-10, 10), # random rotate -45 ~ +45 degree\n",
    "                shear=(-16, 16), # random shear -16 ~ +16 degree\n",
    "                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)} # scale x, y: 80%~120%\n",
    "            ),\n",
    "        ])\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.mask_paths) / self.batch_size)) # batches per epoch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        idxs = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # Find list of IDs\n",
    "        batch_img_paths = [self.img_paths[i] for i in idxs]\n",
    "        batch_mask_paths = [self.mask_paths[i] for i in idxs]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(batch_img_paths, batch_mask_paths)\n",
    "        if self.mode != 'test':\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Updates indexes after each epoch\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, img_paths, mask_paths):\n",
    "        # Generates data containing batch_size samples\n",
    "        x = np.empty((len(img_paths), self.img_size, self.img_size, 3), dtype=np.float32)\n",
    "        y = np.empty((len(img_paths), self.img_size, self.img_size, 1), dtype=np.float32)\n",
    "\n",
    "        for i, (img_path, mask_path) in enumerate(zip(img_paths, mask_paths)):\n",
    "            img = cv2.imread(img_path)[:, :,::-1]\n",
    "            img = self.preprocess(img)\n",
    "            x[i] = img\n",
    "            if self.mode != 'test':\n",
    "                mask = np.array(Image.open(mask_path))\n",
    "                mask = self.preprocess(mask)\n",
    "                y[i] = np.expand_dims(mask, axis=-1)\n",
    "        \n",
    "        # imgaug: augmentation\n",
    "        if self.aug:\n",
    "            x, y = self.seq(images=x, heatmaps=y)\n",
    "        # Binarize Mask\n",
    "        y[y>0] = 1. # 0. or 1.\n",
    "        return x, y\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        data = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        data = data / 255. # normalize to 0~1\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "class FastText():\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 seq_length,\n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 learning_rate,\n",
    "                 learning_decay_rate,\n",
    "                 learning_decay_steps,\n",
    "                 epoch,\n",
    "                 dropout_keep_prob\n",
    "                 ):\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_decay_rate = learning_decay_rate\n",
    "        self.learning_decay_steps = learning_decay_steps\n",
    "        self.epoch = epoch\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.num_classes], name='input_y')\n",
    "        self.model()\n",
    " \n",
    "    def model(self):\n",
    "        # 词向量映射\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.embedding = tf.get_variable(\"embedding\", [self.vocab_size, self.embedding_dim])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(self.embedding, self.input_x)\n",
    " \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            dropout_output = tf.nn.dropout(embedding_inputs, self.dropout_keep_prob)\n",
    " \n",
    "        # 对词向量进行平均\n",
    "        with tf.name_scope(\"average\"):\n",
    "            mean_sentence = tf.reduce_mean(dropout_output, axis=1)\n",
    " \n",
    "        # 输出层\n",
    "        with tf.name_scope(\"score\"):\n",
    "            self.logits = tf.layers.dense(mean_sentence, self.num_classes,name='dense_layer')\n",
    " \n",
    "        # 损失函数\n",
    "        self.loss = cross_entropy_loss(logits=self.logits,labels=self.input_y)\n",
    " \n",
    "        # 优化函数\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
    "                                                   self.learning_decay_steps, self.learning_decay_rate,\n",
    "                                                   staircase=True)\n",
    " \n",
    "        optimizer= tf.train.AdamOptimizer(learning_rate)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        self.optim = slim.learning.create_train_op(total_loss=self.loss, optimizer=optimizer,update_ops=update_ops)\n",
    " \n",
    "        # 准确率\n",
    "        self.acc = accuracy(logits=self.logits,labels=self.input_y)\n",
    " \n",
    "    def fit(self,train_x,train_y,val_x,val_y,batch_size):\n",
    "        # 创建模型保存路径\n",
    "        if not os.path.exists('./saves/fasttext'): os.makedirs('./saves/fasttext')\n",
    "        if not os.path.exists('./train_logs/fasttext'): os.makedirs('./train_logs/fasttext')\n",
    " \n",
    "        # 开始训练\n",
    "        train_steps = 0\n",
    "        best_val_acc = 0\n",
    "        # summary\n",
    "        tf.summary.scalar('val_loss', self.loss)\n",
    "        tf.summary.scalar('val_acc', self.acc)\n",
    "        merged = tf.summary.merge_all()\n",
    " \n",
    "        # 初始化变量\n",
    "        sess = tf.Session()\n",
    "        writer = tf.summary.FileWriter('./train_logs/fasttext', sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=10)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    " \n",
    "        for i in range(self.epoch):\n",
    "            batch_train = self.batch_iter(train_x, train_y, batch_size)\n",
    "            for batch_x,batch_y in batch_train:\n",
    "                train_steps += 1\n",
    "                feed_dict = {self.input_x:batch_x,self.input_y:batch_y}\n",
    "                _, train_loss, train_acc = sess.run([self.optim,self.loss,self.acc],feed_dict=feed_dict)\n",
    " \n",
    "                if train_steps % 1000 == 0:\n",
    "                    feed_dict = {self.input_x:val_x,self.input_y:val_y}\n",
    "                    val_loss,val_acc = sess.run([self.loss,self.acc],feed_dict=feed_dict)\n",
    " \n",
    "                    summary = sess.run(merged,feed_dict=feed_dict)\n",
    "                    writer.add_summary(summary, global_step=train_steps)\n",
    " \n",
    "                    if val_acc>=best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        saver.save(sess, \"./saves/fasttext/\", global_step=train_steps)\n",
    " \n",
    "                    msg = 'epoch:%d/%d,train_steps:%d,train_loss:%.4f,train_acc:%.4f,val_loss:%.4f,val_acc:%.4f'\n",
    "                    print(msg % (i,self.epoch,train_steps,train_loss,train_acc,val_loss,val_acc))\n",
    " \n",
    "    def batch_iter(self, x, y, batch_size=32, shuffle=True):\n",
    "        \"\"\"\n",
    "        生成batch数据\n",
    "        :param x: 训练集特征变量\n",
    "        :param y: 训练集标签\n",
    "        :param batch_size: 每个batch的大小\n",
    "        :param shuffle: 是否在每个epoch时打乱数据\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        data_len = len(x)\n",
    "        num_batch = int((data_len - 1) / batch_size) + 1\n",
    " \n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_len))\n",
    "            x_shuffle = x[shuffle_indices]\n",
    "            y_shuffle = y[shuffle_indices]\n",
    "        else:\n",
    "            x_shuffle = x\n",
    "            y_shuffle = y\n",
    "        for i in range(num_batch):\n",
    "            start_index = i * batch_size\n",
    "            end_index = min((i + 1) * batch_size, data_len)\n",
    "            yield (x_shuffle[start_index:end_index], y_shuffle[start_index:end_index])\n",
    " \n",
    "    def predict(self,x):\n",
    "        sess = tf.Session()\n",
    " \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state('./saves/fasttext/')\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    " \n",
    "        feed_dict = {self.input_x: x}\n",
    "        logits = sess.run(self.logits, feed_dict=feed_dict)\n",
    "        y_pred = np.argmax(logits, 1)\n",
    "        return y_pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3688bbe5d71ec827a181cc1b6e9c5199ce963ff51a52c02cbea4adac451cd3ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
